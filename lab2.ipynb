{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Sentence #           Word Tag\n",
      "0        Sentence: 1      Thousands   O\n",
      "1                NaN             of   O\n",
      "2                NaN  demonstrators   O\n",
      "3                NaN           have   O\n",
      "4                NaN        marched   O\n",
      "...              ...            ...  ..\n",
      "1048570          NaN           they   O\n",
      "1048571          NaN      responded   O\n",
      "1048572          NaN             to   O\n",
      "1048573          NaN            the   O\n",
      "1048574          NaN         attack   O\n",
      "\n",
      "[1048575 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('ner_dataset.csv', encoding= 'unicode_escape', usecols=['Sentence #','Word','Tag'])\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```Data Cleaning```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the dataset using pd.read_csv() and clean the dataset.\n",
    "\n",
    "To avoid using very short or long sentences, select the ones with more than 15 and fewer than 30 words.\n",
    "\n",
    "Split the data into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sentence #           Word Tag\n",
      "0            Sentence: 1      Thousands   O\n",
      "1            Sentence: 1             of   O\n",
      "2            Sentence: 1  demonstrators   O\n",
      "3            Sentence: 1           have   O\n",
      "4            Sentence: 1        marched   O\n",
      "...                  ...            ...  ..\n",
      "1048570  Sentence: 47959           they   O\n",
      "1048571  Sentence: 47959      responded   O\n",
      "1048572  Sentence: 47959             to   O\n",
      "1048573  Sentence: 47959            the   O\n",
      "1048574  Sentence: 47959         attack   O\n",
      "\n",
      "[1048575 rows x 3 columns]\n",
      "         Sentence #           Word Tag\n",
      "0                 1      Thousands   O\n",
      "1                 1             of   O\n",
      "2                 1  demonstrators   O\n",
      "3                 1           have   O\n",
      "4                 1        marched   O\n",
      "...             ...            ...  ..\n",
      "1048570       47959           they   O\n",
      "1048571       47959      responded   O\n",
      "1048572       47959             to   O\n",
      "1048573       47959            the   O\n",
      "1048574       47959         attack   O\n",
      "\n",
      "[1048575 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Word Tag\n",
       "Sentence #                   \n",
       "1               Thousands   O\n",
       "1                      of   O\n",
       "1           demonstrators   O\n",
       "1                    have   O\n",
       "1                 marched   O"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fill Nan values using the previous value. \n",
    "# This way we can separate each sentence.\n",
    "df.fillna(method='ffill', inplace=True)\n",
    "print(df)\n",
    "# Remove \"Sentence: \" from the sentence column\n",
    "df['Sentence #'] = df['Sentence #'].str.replace('Sentence: ','').astype(int)\n",
    "print(df)\n",
    "# Set the index as the sentence number. Then we filter each sentence, joininig the text\n",
    "df.set_index('Sentence #', inplace=True)\n",
    "# Take a quick look at the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    1     2     3 ... 47957 47958 47959]\n",
      "[24 30 14 ... 11 11  8]\n",
      "<class 'numpy.ndarray'>\n",
      "[ True False False ... False False False]\n",
      "                     Word Tag\n",
      "Sentence #                   \n",
      "1               Thousands   O\n",
      "1                      of   O\n",
      "1           demonstrators   O\n",
      "1                    have   O\n",
      "1                 marched   O\n",
      "...                   ...  ..\n",
      "47959                they   O\n",
      "47959           responded   O\n",
      "47959                  to   O\n",
      "47959                 the   O\n",
      "47959              attack   O\n",
      "\n",
      "[1048575 rows x 2 columns]\n",
      "[    1     5     6 ... 47954 47955 47956]\n",
      "                     Word Tag\n",
      "Sentence #                   \n",
      "1               Thousands   O\n",
      "1                      of   O\n",
      "1           demonstrators   O\n",
      "1                    have   O\n",
      "1                 marched   O\n",
      "...                   ...  ..\n",
      "47956                   a   O\n",
      "47956              border   O\n",
      "47956            security   O\n",
      "47956             outpost   O\n",
      "47956                   .   O\n",
      "\n",
      "[653765 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe by sequence length\n",
    "# As you can see from the head above, the index values are repeated hence we need to know all the unique indices in our dataframe.\n",
    "# Each unique index represents a sentence number.\n",
    "# We also want to find the length of each sentence in the dataframe hence we make use of the unique function.\n",
    "index, length = np.unique(df.index, return_counts=True)\n",
    "print(index)\n",
    "print(length)\n",
    "print(type(length))\n",
    "# To avoid using long sentences, we are restricting length between 15 to 30 words\n",
    "b1 = length>15\n",
    "b2 = length<30\n",
    "\n",
    "# We are using logical_and function because lets say if a sentence's length is greater than 15 and also greater than 30, we won't consider that sentence.\n",
    "# For a sentence to be considered , both conditions must be true.\n",
    "b = np.logical_and(b1, b2)\n",
    "print(b)\n",
    "\n",
    "# Taking sentences satisfying the condition.\n",
    "index = index[b]\n",
    "length = length[b]\n",
    "\n",
    "\n",
    "# Constucting a dataframe out of valid sentences.\n",
    "print(df)\n",
    "df = df.loc[index]\n",
    "print(index)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code for train-test split\n",
    "index = np.unique(df.index)\n",
    "\n",
    "# Splitting sentences into training and test sets using index values.\n",
    "# We are using random_state for reproducibility\n",
    "train_index, val_index = train_test_split(index, train_size=0.8,random_state=0)\n",
    "\n",
    "df_train = df.loc[train_index]\n",
    "df_val = df.loc[val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take a quick look at the different tags\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(df_train.Tag, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ```Data Transformation```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We know that words need to be transformed into numerical form. Transform the tags and words into numbers.\n",
    "\n",
    "* Perform label encoding on the tags. \n",
    "\n",
    "* Remove non-alphanumeric characters from the word column in both the train and validation sets and replace them with an empty string.\n",
    "\n",
    "* Tokenize the word column using tf.keras.preprocessing.text.Tokenizer with default parameters.\n",
    "\n",
    "* After tokenization of the word column, remove empty elements from the tokens created.\n",
    "\n",
    "* After doing the above steps, we will now create our training and validation data.\n",
    "\n",
    "  * Create three lists:\n",
    "\n",
    "    * A list of sentences, where each element is another list of all the tokens of the words in each sentence.\n",
    "\n",
    "    * A list of tags, where each element is another list of all the tags corresponding to the words in each sentence extracted.\n",
    "\n",
    "    * A list of to store the length of sentences.\n",
    "\n",
    "  * Omit any sentence which only has a single element.\n",
    "\n",
    "  * Ensure that the sentences are more than 15 and fewer than 30 words. Keep data points that satisfy this condition.\n",
    "\n",
    "  * Post pad the sentences using `tf.keras.utils.pad_sequences()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Word  Tag\n",
      "Sentence #             \n",
      "9750           [1]   16\n",
      "9750         [147]   16\n",
      "9750        [2322]   16\n",
      "9750        [3605]   16\n",
      "9750        [2486]   16\n",
      "...            ...  ...\n",
      "4507         [513]   16\n",
      "4507           [6]   16\n",
      "4507        [5330]   16\n",
      "4507        [2789]   16\n",
      "4507            []   16\n",
      "\n",
      "[523321 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Label encoding on the tags:\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train['Tag'])\n",
    "df_train['Tag'] = le.transform(df_train['Tag'])\n",
    "df_val['Tag'] = le.transform(df_val['Tag'])\n",
    "\n",
    "# remove non-alphabetic characters in Word column\n",
    "def clean_text(text):\n",
    "    text = re.sub('[^A-Za-z0-9]', '', text)\n",
    "    return text\n",
    "df_train['Word'] = df_train['Word'].apply(clean_text)\n",
    "df_val['Word'] = df_val['Word'].apply(clean_text)\n",
    "# tokenize the word column using keras tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df_train['Word'])\n",
    "df_train['Word'] = tokenizer.texts_to_sequences(df_train['Word'])\n",
    "df_val['Word'] = tokenizer.texts_to_sequences(df_val['Word'])\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/m3tcrtn96xs17l0rtk490mg00000gn/T/ipykernel_14401/4177708169.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  word_list = np.array([[val[0] for val in df['Word'].loc[index]] for index in indexs])\n",
      "/var/folders/dp/m3tcrtn96xs17l0rtk490mg00000gn/T/ipykernel_14401/4177708169.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tag_list = np.array([[val for val in df['Tag'].loc[index]] for index in indexs])\n"
     ]
    }
   ],
   "source": [
    "# remove empty sequences\n",
    "df_train = df_train[df_train['Word'].map(len) > 0]\n",
    "df_val = df_val[df_val['Word'].map(len) > 0]\n",
    "\n",
    "# create training and validation data:\n",
    "# 1st: list of lists of word tokens; 2ed: list of lists of tag tokens; 3rd: list of sentence lengths\n",
    "def createData(df, indexs):\n",
    "    word_list = np.array([[val[0] for val in df['Word'].loc[index]] for index in indexs])\n",
    "    tag_list = np.array([[val for val in df['Tag'].loc[index]] for index in indexs])\n",
    "    length_list = np.array([len(val) for val in word_list])\n",
    "    # ensure sentences lengths are between 15 and 30\n",
    "    b1 = length_list>15\n",
    "    b2 = length_list<30\n",
    "    b = np.logical_and(b1, b2)\n",
    "    word_list = word_list[b]\n",
    "    tag_list = tag_list[b]\n",
    "    length_list = length_list[b]\n",
    "    # pad sequences using keras.utils.pad_sequences\n",
    "    word_list = tf.keras.preprocessing.sequence.pad_sequences(word_list, padding='post')\n",
    "    tag_list = tf.keras.preprocessing.sequence.pad_sequences(tag_list, padding='post')\n",
    "    return word_list, tag_list, length_list\n",
    "word_train, tag_train, length_train = createData(df_train, train_index)\n",
    "word_val, tag_val, length_val = createData(df_val, val_index)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>[1]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>[147]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>[2322]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>[3605]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9750</th>\n",
       "      <td>[2486]</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Word  Tag\n",
       "Sentence #             \n",
       "9750           [1]   16\n",
       "9750         [147]   16\n",
       "9750        [2322]   16\n",
       "9750        [3605]   16\n",
       "9750        [2486]   16"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  0  0  0  0  0\n",
      "   0  0  0  0]\n",
      " [16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  7  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0]]\n",
      "[[  106   194  1514   141  6457  3232     5   105   318     6 15874  4172\n",
      "     13  3421   810   384   141     3   211     0     0     0     0     0\n",
      "      0     0     0     0]\n",
      " [   48    28    20     1   359    18   601   396    14    30   287     2\n",
      "      1    88     3    67     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "[[16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16  0\n",
      "   0  0  0  0]\n",
      " [16  3 16 16 16 16 16 16  7 16 16  2 16 16 16 16 16 16 16  3  6 14 14 14\n",
      "   0  0  0  0]]\n",
      "[[    1   147  2322  3605  2486    70     8     1   116     6   514    12\n",
      "     26   153  2678 13078  1842     6   299     4   789   180   110     0\n",
      "      0     0     0     0]\n",
      " [    1   419   123    18   286     4  6148     5  1757   669     4   501\n",
      "     64   340    43   445     5   185    14   175    97    54  1342   998\n",
      "      0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(tag_val[:2])\n",
    "print(word_val[:2])\n",
    "print(tag_train[:2])\n",
    "print(word_train[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create 4 many-to-many models with different values of h = [8,16,32].\n",
    "\n",
    "  * A single-layer RNN network.\n",
    "\n",
    "  * A k-layer RNN network, where k=[2,3]\n",
    "\n",
    "  * A bidirectional single-layer RNN. \n",
    "\n",
    "  * A bidirectional k-layer RNN, where k=[2,3]\n",
    "\n",
    "* In each model, \n",
    "\n",
    "  * The first layer should be Embedding, with mask_zero=True. This will propagate the mask in the network.\n",
    "\n",
    "  * Then, SimpleRNN layers as required by the model. Remember to set return_sequences=True so as to build a many-to-many RNN models. \n",
    "\n",
    "  * For the bidirectional RNNs, remember to set go_backwards=True for the backward RNN and ensure to concatenate the hidden layers in the bidirectional RNN as needed.\n",
    "\n",
    "  * Define a dense layer, with size set as the number of classes and activation softmax.\n",
    "\n",
    "  * Note that the loss function to be used after label encoding is `sparse_categorical_crossentropy`.\n",
    "\n",
    "  * The metric to be used is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "166/166 [==============================] - 3s 10ms/step - loss: 1.6602 - accuracy: 0.7789 - val_loss: 0.9818 - val_accuracy: 0.8365\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.8971 - accuracy: 0.8309 - val_loss: 0.8166 - val_accuracy: 0.8365\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.7885 - accuracy: 0.8312 - val_loss: 0.7012 - val_accuracy: 0.8391\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.6375 - accuracy: 0.8415 - val_loss: 0.5478 - val_accuracy: 0.8596\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.5214 - accuracy: 0.8644 - val_loss: 0.4672 - val_accuracy: 0.8783\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.4512 - accuracy: 0.8807 - val_loss: 0.4084 - val_accuracy: 0.8986\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.3933 - accuracy: 0.9049 - val_loss: 0.3564 - val_accuracy: 0.9177\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.3437 - accuracy: 0.9194 - val_loss: 0.3147 - val_accuracy: 0.9276\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.3053 - accuracy: 0.9286 - val_loss: 0.2848 - val_accuracy: 0.9352\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.2768 - accuracy: 0.9347 - val_loss: 0.2634 - val_accuracy: 0.9386\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.2550 - accuracy: 0.9395 - val_loss: 0.2473 - val_accuracy: 0.9415\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.2376 - accuracy: 0.9429 - val_loss: 0.2347 - val_accuracy: 0.9430\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.2232 - accuracy: 0.9452 - val_loss: 0.2244 - val_accuracy: 0.9441\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.2108 - accuracy: 0.9468 - val_loss: 0.2159 - val_accuracy: 0.9451\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.1997 - accuracy: 0.9483 - val_loss: 0.2085 - val_accuracy: 0.9459\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.1897 - accuracy: 0.9495 - val_loss: 0.2020 - val_accuracy: 0.9461\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.1809 - accuracy: 0.9504 - val_loss: 0.1964 - val_accuracy: 0.9467\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.1729 - accuracy: 0.9522 - val_loss: 0.1915 - val_accuracy: 0.9480\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 1s 7ms/step - loss: 0.1661 - accuracy: 0.9539 - val_loss: 0.1877 - val_accuracy: 0.9490\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1601 - accuracy: 0.9548 - val_loss: 0.1843 - val_accuracy: 0.9493\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 4s 14ms/step - loss: 1.6846 - accuracy: 0.7367 - val_loss: 1.0254 - val_accuracy: 0.8365\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.9095 - accuracy: 0.8309 - val_loss: 0.8218 - val_accuracy: 0.8367\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.7817 - accuracy: 0.8324 - val_loss: 0.6359 - val_accuracy: 0.8443\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.5580 - accuracy: 0.8625 - val_loss: 0.4777 - val_accuracy: 0.8800\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.4551 - accuracy: 0.8792 - val_loss: 0.4175 - val_accuracy: 0.8854\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.3993 - accuracy: 0.8915 - val_loss: 0.3706 - val_accuracy: 0.9044\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.3500 - accuracy: 0.9099 - val_loss: 0.3289 - val_accuracy: 0.9179\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.3074 - accuracy: 0.9204 - val_loss: 0.2964 - val_accuracy: 0.9235\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.2728 - accuracy: 0.9307 - val_loss: 0.2712 - val_accuracy: 0.9311\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.2446 - accuracy: 0.9383 - val_loss: 0.2527 - val_accuracy: 0.9348\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.2218 - accuracy: 0.9443 - val_loss: 0.2389 - val_accuracy: 0.9396\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.2033 - accuracy: 0.9499 - val_loss: 0.2294 - val_accuracy: 0.9425\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1884 - accuracy: 0.9538 - val_loss: 0.2233 - val_accuracy: 0.9442\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1761 - accuracy: 0.9569 - val_loss: 0.2179 - val_accuracy: 0.9456\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1658 - accuracy: 0.9592 - val_loss: 0.2155 - val_accuracy: 0.9457\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1574 - accuracy: 0.9607 - val_loss: 0.2130 - val_accuracy: 0.9461\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1500 - accuracy: 0.9620 - val_loss: 0.2136 - val_accuracy: 0.9452\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1435 - accuracy: 0.9631 - val_loss: 0.2129 - val_accuracy: 0.9454\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1382 - accuracy: 0.9640 - val_loss: 0.2131 - val_accuracy: 0.9452\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1334 - accuracy: 0.9648 - val_loss: 0.2134 - val_accuracy: 0.9452\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 6s 18ms/step - loss: 1.4300 - accuracy: 0.7944 - val_loss: 0.9375 - val_accuracy: 0.8365\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.8858 - accuracy: 0.8310 - val_loss: 0.8230 - val_accuracy: 0.8371\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.8211 - accuracy: 0.8328 - val_loss: 0.7781 - val_accuracy: 0.8395\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.6583 - accuracy: 0.8389 - val_loss: 0.5031 - val_accuracy: 0.8608\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.4649 - accuracy: 0.8735 - val_loss: 0.4163 - val_accuracy: 0.8842\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.3933 - accuracy: 0.8919 - val_loss: 0.3664 - val_accuracy: 0.9047\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.3459 - accuracy: 0.9056 - val_loss: 0.3332 - val_accuracy: 0.9099\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.3097 - accuracy: 0.9157 - val_loss: 0.3077 - val_accuracy: 0.9216\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.2807 - accuracy: 0.9257 - val_loss: 0.2897 - val_accuracy: 0.9254\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.2565 - accuracy: 0.9320 - val_loss: 0.2761 - val_accuracy: 0.9294\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.2356 - accuracy: 0.9395 - val_loss: 0.2643 - val_accuracy: 0.9334\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.2174 - accuracy: 0.9459 - val_loss: 0.2550 - val_accuracy: 0.9368\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.2017 - accuracy: 0.9510 - val_loss: 0.2477 - val_accuracy: 0.9389\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.1881 - accuracy: 0.9549 - val_loss: 0.2433 - val_accuracy: 0.9405\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.1768 - accuracy: 0.9577 - val_loss: 0.2394 - val_accuracy: 0.9413\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.1669 - accuracy: 0.9600 - val_loss: 0.2374 - val_accuracy: 0.9417\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.1583 - accuracy: 0.9617 - val_loss: 0.2362 - val_accuracy: 0.9419\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.1507 - accuracy: 0.9634 - val_loss: 0.2347 - val_accuracy: 0.9424\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.1448 - accuracy: 0.9646 - val_loss: 0.2368 - val_accuracy: 0.9408\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.1394 - accuracy: 0.9655 - val_loss: 0.2349 - val_accuracy: 0.9421\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 3s 10ms/step - loss: 1.3688 - accuracy: 0.7285 - val_loss: 0.8274 - val_accuracy: 0.8366\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 9ms/step - loss: 0.7672 - accuracy: 0.8322 - val_loss: 0.6181 - val_accuracy: 0.8439\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 1s 9ms/step - loss: 0.5122 - accuracy: 0.8703 - val_loss: 0.4064 - val_accuracy: 0.8975\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 1s 9ms/step - loss: 0.3706 - accuracy: 0.9025 - val_loss: 0.3197 - val_accuracy: 0.9192\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.2979 - accuracy: 0.9253 - val_loss: 0.2674 - val_accuracy: 0.9346\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.2488 - accuracy: 0.9370 - val_loss: 0.2319 - val_accuracy: 0.9412\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.2148 - accuracy: 0.9444 - val_loss: 0.2094 - val_accuracy: 0.9463\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1911 - accuracy: 0.9496 - val_loss: 0.1947 - val_accuracy: 0.9483\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1739 - accuracy: 0.9530 - val_loss: 0.1848 - val_accuracy: 0.9499\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1605 - accuracy: 0.9555 - val_loss: 0.1781 - val_accuracy: 0.9512\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1502 - accuracy: 0.9573 - val_loss: 0.1738 - val_accuracy: 0.9515\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1417 - accuracy: 0.9588 - val_loss: 0.1706 - val_accuracy: 0.9520\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1346 - accuracy: 0.9600 - val_loss: 0.1682 - val_accuracy: 0.9522\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1285 - accuracy: 0.9611 - val_loss: 0.1668 - val_accuracy: 0.9522\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1234 - accuracy: 0.9622 - val_loss: 0.1665 - val_accuracy: 0.9522\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1188 - accuracy: 0.9631 - val_loss: 0.1659 - val_accuracy: 0.9525\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1146 - accuracy: 0.9642 - val_loss: 0.1663 - val_accuracy: 0.9526\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1109 - accuracy: 0.9653 - val_loss: 0.1668 - val_accuracy: 0.9520\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1076 - accuracy: 0.9660 - val_loss: 0.1680 - val_accuracy: 0.9524\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 1s 8ms/step - loss: 0.1042 - accuracy: 0.9671 - val_loss: 0.1685 - val_accuracy: 0.9517\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 4s 14ms/step - loss: 1.1555 - accuracy: 0.8082 - val_loss: 0.8207 - val_accuracy: 0.8366\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.7433 - accuracy: 0.8339 - val_loss: 0.5566 - val_accuracy: 0.8496\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.4708 - accuracy: 0.8678 - val_loss: 0.3912 - val_accuracy: 0.8987\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.3561 - accuracy: 0.9051 - val_loss: 0.3155 - val_accuracy: 0.9185\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.2806 - accuracy: 0.9264 - val_loss: 0.2589 - val_accuracy: 0.9346\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.2229 - accuracy: 0.9429 - val_loss: 0.2228 - val_accuracy: 0.9433\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1832 - accuracy: 0.9535 - val_loss: 0.2025 - val_accuracy: 0.9484\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.1563 - accuracy: 0.9594 - val_loss: 0.1927 - val_accuracy: 0.9492\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.1381 - accuracy: 0.9633 - val_loss: 0.1895 - val_accuracy: 0.9494\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.1254 - accuracy: 0.9660 - val_loss: 0.1883 - val_accuracy: 0.9500\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1155 - accuracy: 0.9683 - val_loss: 0.1875 - val_accuracy: 0.9503\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1076 - accuracy: 0.9700 - val_loss: 0.1911 - val_accuracy: 0.9497\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1013 - accuracy: 0.9717 - val_loss: 0.1910 - val_accuracy: 0.9498\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0962 - accuracy: 0.9730 - val_loss: 0.1943 - val_accuracy: 0.9481\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0909 - accuracy: 0.9744 - val_loss: 0.1979 - val_accuracy: 0.9482\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0867 - accuracy: 0.9755 - val_loss: 0.2016 - val_accuracy: 0.9471\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0826 - accuracy: 0.9764 - val_loss: 0.2054 - val_accuracy: 0.9469\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0790 - accuracy: 0.9775 - val_loss: 0.2080 - val_accuracy: 0.9459\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0758 - accuracy: 0.9786 - val_loss: 0.2115 - val_accuracy: 0.9458\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0727 - accuracy: 0.9794 - val_loss: 0.2146 - val_accuracy: 0.9451\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 7s 20ms/step - loss: 1.2600 - accuracy: 0.7766 - val_loss: 0.8220 - val_accuracy: 0.8364\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 3s 17ms/step - loss: 0.8029 - accuracy: 0.8315 - val_loss: 0.6787 - val_accuracy: 0.8404\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.5126 - accuracy: 0.8659 - val_loss: 0.3983 - val_accuracy: 0.9002\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.3557 - accuracy: 0.9081 - val_loss: 0.3137 - val_accuracy: 0.9221\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.2816 - accuracy: 0.9282 - val_loss: 0.2700 - val_accuracy: 0.9319\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.2333 - accuracy: 0.9413 - val_loss: 0.2454 - val_accuracy: 0.9394\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.1971 - accuracy: 0.9518 - val_loss: 0.2268 - val_accuracy: 0.9442\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 3s 17ms/step - loss: 0.1687 - accuracy: 0.9594 - val_loss: 0.2174 - val_accuracy: 0.9455\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 3s 17ms/step - loss: 0.1467 - accuracy: 0.9642 - val_loss: 0.2099 - val_accuracy: 0.9460\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.1293 - accuracy: 0.9681 - val_loss: 0.2072 - val_accuracy: 0.9476\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.1156 - accuracy: 0.9712 - val_loss: 0.2093 - val_accuracy: 0.9464\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.1052 - accuracy: 0.9734 - val_loss: 0.2125 - val_accuracy: 0.9454\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0966 - accuracy: 0.9753 - val_loss: 0.2151 - val_accuracy: 0.9448\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0894 - accuracy: 0.9771 - val_loss: 0.2200 - val_accuracy: 0.9437\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0837 - accuracy: 0.9782 - val_loss: 0.2244 - val_accuracy: 0.9420\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0787 - accuracy: 0.9795 - val_loss: 0.2301 - val_accuracy: 0.9425\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0744 - accuracy: 0.9805 - val_loss: 0.2355 - val_accuracy: 0.9430\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0707 - accuracy: 0.9814 - val_loss: 0.2407 - val_accuracy: 0.9409\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 3s 17ms/step - loss: 0.0673 - accuracy: 0.9823 - val_loss: 0.2450 - val_accuracy: 0.9406\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.0648 - accuracy: 0.9827 - val_loss: 0.2492 - val_accuracy: 0.9402\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 3s 12ms/step - loss: 1.0733 - accuracy: 0.8099 - val_loss: 0.7674 - val_accuracy: 0.8375\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.5753 - accuracy: 0.8596 - val_loss: 0.3679 - val_accuracy: 0.9096\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.3032 - accuracy: 0.9265 - val_loss: 0.2345 - val_accuracy: 0.9443\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.2133 - accuracy: 0.9455 - val_loss: 0.1913 - val_accuracy: 0.9498\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1734 - accuracy: 0.9526 - val_loss: 0.1730 - val_accuracy: 0.9517\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.1502 - accuracy: 0.9567 - val_loss: 0.1639 - val_accuracy: 0.9529\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.1344 - accuracy: 0.9597 - val_loss: 0.1593 - val_accuracy: 0.9532\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.1226 - accuracy: 0.9621 - val_loss: 0.1586 - val_accuracy: 0.9536\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.1131 - accuracy: 0.9645 - val_loss: 0.1582 - val_accuracy: 0.9540\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.1050 - accuracy: 0.9669 - val_loss: 0.1602 - val_accuracy: 0.9530\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0981 - accuracy: 0.9690 - val_loss: 0.1626 - val_accuracy: 0.9532\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0920 - accuracy: 0.9711 - val_loss: 0.1679 - val_accuracy: 0.9527\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0866 - accuracy: 0.9725 - val_loss: 0.1708 - val_accuracy: 0.9512\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0820 - accuracy: 0.9742 - val_loss: 0.1745 - val_accuracy: 0.9512\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0777 - accuracy: 0.9756 - val_loss: 0.1782 - val_accuracy: 0.9508\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0736 - accuracy: 0.9769 - val_loss: 0.1837 - val_accuracy: 0.9491\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 2s 10ms/step - loss: 0.0699 - accuracy: 0.9781 - val_loss: 0.1915 - val_accuracy: 0.9490\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0666 - accuracy: 0.9793 - val_loss: 0.1966 - val_accuracy: 0.9487\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0637 - accuracy: 0.9802 - val_loss: 0.1993 - val_accuracy: 0.9487\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 2s 11ms/step - loss: 0.0611 - accuracy: 0.9810 - val_loss: 0.2067 - val_accuracy: 0.9483\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 5s 16ms/step - loss: 1.0102 - accuracy: 0.8074 - val_loss: 0.7465 - val_accuracy: 0.8379\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.5071 - accuracy: 0.8722 - val_loss: 0.3272 - val_accuracy: 0.9177\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.2729 - accuracy: 0.9311 - val_loss: 0.2266 - val_accuracy: 0.9442\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.1939 - accuracy: 0.9497 - val_loss: 0.1932 - val_accuracy: 0.9484\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.1542 - accuracy: 0.9583 - val_loss: 0.1811 - val_accuracy: 0.9502\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.1296 - accuracy: 0.9639 - val_loss: 0.1791 - val_accuracy: 0.9497\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.1116 - accuracy: 0.9683 - val_loss: 0.1805 - val_accuracy: 0.9496\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0970 - accuracy: 0.9721 - val_loss: 0.1852 - val_accuracy: 0.9479\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0858 - accuracy: 0.9754 - val_loss: 0.1918 - val_accuracy: 0.9471\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0759 - accuracy: 0.9783 - val_loss: 0.1976 - val_accuracy: 0.9477\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0684 - accuracy: 0.9803 - val_loss: 0.2048 - val_accuracy: 0.9467\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0625 - accuracy: 0.9818 - val_loss: 0.2144 - val_accuracy: 0.9458\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.0575 - accuracy: 0.9832 - val_loss: 0.2206 - val_accuracy: 0.9453\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0534 - accuracy: 0.9844 - val_loss: 0.2266 - val_accuracy: 0.9440\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0500 - accuracy: 0.9852 - val_loss: 0.2368 - val_accuracy: 0.9429\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0469 - accuracy: 0.9860 - val_loss: 0.2428 - val_accuracy: 0.9427\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0440 - accuracy: 0.9870 - val_loss: 0.2503 - val_accuracy: 0.9416\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0417 - accuracy: 0.9876 - val_loss: 0.2578 - val_accuracy: 0.9413\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.0397 - accuracy: 0.9881 - val_loss: 0.2624 - val_accuracy: 0.9413\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.0382 - accuracy: 0.9886 - val_loss: 0.2725 - val_accuracy: 0.9393\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 6s 22ms/step - loss: 0.9587 - accuracy: 0.8147 - val_loss: 0.7470 - val_accuracy: 0.8394\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.4774 - accuracy: 0.8737 - val_loss: 0.2995 - val_accuracy: 0.9107\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.2539 - accuracy: 0.9329 - val_loss: 0.2213 - val_accuracy: 0.9449\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.1812 - accuracy: 0.9529 - val_loss: 0.1972 - val_accuracy: 0.9485\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.1410 - accuracy: 0.9621 - val_loss: 0.1924 - val_accuracy: 0.9480\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.1142 - accuracy: 0.9686 - val_loss: 0.1957 - val_accuracy: 0.9479\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0960 - accuracy: 0.9733 - val_loss: 0.2044 - val_accuracy: 0.9452\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0828 - accuracy: 0.9767 - val_loss: 0.2133 - val_accuracy: 0.9455\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0735 - accuracy: 0.9793 - val_loss: 0.2199 - val_accuracy: 0.9452\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0658 - accuracy: 0.9812 - val_loss: 0.2325 - val_accuracy: 0.9428\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0597 - accuracy: 0.9830 - val_loss: 0.2388 - val_accuracy: 0.9429\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0547 - accuracy: 0.9843 - val_loss: 0.2496 - val_accuracy: 0.9425\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0504 - accuracy: 0.9855 - val_loss: 0.2619 - val_accuracy: 0.9389\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0472 - accuracy: 0.9862 - val_loss: 0.2621 - val_accuracy: 0.9413\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0440 - accuracy: 0.9872 - val_loss: 0.2717 - val_accuracy: 0.9406\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 3s 20ms/step - loss: 0.0416 - accuracy: 0.9877 - val_loss: 0.2788 - val_accuracy: 0.9389\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0392 - accuracy: 0.9884 - val_loss: 0.2885 - val_accuracy: 0.9386\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0380 - accuracy: 0.9887 - val_loss: 0.2923 - val_accuracy: 0.9377\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0359 - accuracy: 0.9893 - val_loss: 0.3012 - val_accuracy: 0.9369\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0344 - accuracy: 0.9897 - val_loss: 0.3066 - val_accuracy: 0.9372\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 5s 16ms/step - loss: 1.4771 - accuracy: 0.7587 - val_loss: 0.7590 - val_accuracy: 0.8365\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.6665 - accuracy: 0.8378 - val_loss: 0.5351 - val_accuracy: 0.8605\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.4807 - accuracy: 0.8741 - val_loss: 0.3931 - val_accuracy: 0.8982\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.3663 - accuracy: 0.9003 - val_loss: 0.3092 - val_accuracy: 0.9171\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.2919 - accuracy: 0.9230 - val_loss: 0.2541 - val_accuracy: 0.9372\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.2416 - accuracy: 0.9380 - val_loss: 0.2201 - val_accuracy: 0.9441\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.2090 - accuracy: 0.9455 - val_loss: 0.1999 - val_accuracy: 0.9477\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1870 - accuracy: 0.9503 - val_loss: 0.1869 - val_accuracy: 0.9501\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1708 - accuracy: 0.9540 - val_loss: 0.1780 - val_accuracy: 0.9516\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1580 - accuracy: 0.9567 - val_loss: 0.1715 - val_accuracy: 0.9528\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1475 - accuracy: 0.9591 - val_loss: 0.1666 - val_accuracy: 0.9541\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1386 - accuracy: 0.9611 - val_loss: 0.1634 - val_accuracy: 0.9544\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1312 - accuracy: 0.9628 - val_loss: 0.1611 - val_accuracy: 0.9553\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1247 - accuracy: 0.9644 - val_loss: 0.1595 - val_accuracy: 0.9560\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1192 - accuracy: 0.9657 - val_loss: 0.1586 - val_accuracy: 0.9562\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1143 - accuracy: 0.9669 - val_loss: 0.1581 - val_accuracy: 0.9566\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1100 - accuracy: 0.9679 - val_loss: 0.1583 - val_accuracy: 0.9565\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1061 - accuracy: 0.9688 - val_loss: 0.1580 - val_accuracy: 0.9566\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.1025 - accuracy: 0.9698 - val_loss: 0.1590 - val_accuracy: 0.9565\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.0993 - accuracy: 0.9707 - val_loss: 0.1592 - val_accuracy: 0.9568\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 7s 22ms/step - loss: 1.2997 - accuracy: 0.7546 - val_loss: 0.7863 - val_accuracy: 0.8365\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.6835 - accuracy: 0.8387 - val_loss: 0.5197 - val_accuracy: 0.8668\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.4437 - accuracy: 0.8840 - val_loss: 0.3440 - val_accuracy: 0.9165\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.3069 - accuracy: 0.9232 - val_loss: 0.2557 - val_accuracy: 0.9370\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.2352 - accuracy: 0.9397 - val_loss: 0.2128 - val_accuracy: 0.9453\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.1950 - accuracy: 0.9490 - val_loss: 0.1911 - val_accuracy: 0.9498\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1696 - accuracy: 0.9549 - val_loss: 0.1788 - val_accuracy: 0.9524\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1515 - accuracy: 0.9595 - val_loss: 0.1721 - val_accuracy: 0.9534\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1376 - accuracy: 0.9631 - val_loss: 0.1681 - val_accuracy: 0.9546\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1262 - accuracy: 0.9658 - val_loss: 0.1661 - val_accuracy: 0.9555\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1166 - accuracy: 0.9683 - val_loss: 0.1668 - val_accuracy: 0.9553\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1082 - accuracy: 0.9704 - val_loss: 0.1678 - val_accuracy: 0.9554\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.1009 - accuracy: 0.9722 - val_loss: 0.1699 - val_accuracy: 0.9553\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0945 - accuracy: 0.9741 - val_loss: 0.1723 - val_accuracy: 0.9550\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0887 - accuracy: 0.9757 - val_loss: 0.1764 - val_accuracy: 0.9542\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0838 - accuracy: 0.9771 - val_loss: 0.1797 - val_accuracy: 0.9541\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0792 - accuracy: 0.9784 - val_loss: 0.1850 - val_accuracy: 0.9531\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 3s 19ms/step - loss: 0.0751 - accuracy: 0.9796 - val_loss: 0.1873 - val_accuracy: 0.9527\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 3s 20ms/step - loss: 0.0714 - accuracy: 0.9807 - val_loss: 0.1931 - val_accuracy: 0.9516\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 3s 21ms/step - loss: 0.0679 - accuracy: 0.9818 - val_loss: 0.1978 - val_accuracy: 0.9512\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 10s 29ms/step - loss: 1.1507 - accuracy: 0.7912 - val_loss: 0.7979 - val_accuracy: 0.8365\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 4s 24ms/step - loss: 0.7399 - accuracy: 0.8323 - val_loss: 0.5797 - val_accuracy: 0.8476\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 4s 24ms/step - loss: 0.4563 - accuracy: 0.8751 - val_loss: 0.3446 - val_accuracy: 0.9070\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 4s 25ms/step - loss: 0.3027 - accuracy: 0.9217 - val_loss: 0.2593 - val_accuracy: 0.9373\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 4s 25ms/step - loss: 0.2286 - accuracy: 0.9446 - val_loss: 0.2158 - val_accuracy: 0.9479\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 4s 25ms/step - loss: 0.1853 - accuracy: 0.9549 - val_loss: 0.1935 - val_accuracy: 0.9512\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.1576 - accuracy: 0.9610 - val_loss: 0.1806 - val_accuracy: 0.9535\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.1385 - accuracy: 0.9652 - val_loss: 0.1741 - val_accuracy: 0.9546\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.1244 - accuracy: 0.9682 - val_loss: 0.1726 - val_accuracy: 0.9552\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.1135 - accuracy: 0.9708 - val_loss: 0.1707 - val_accuracy: 0.9548\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.1043 - accuracy: 0.9728 - val_loss: 0.1718 - val_accuracy: 0.9550\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 5s 31ms/step - loss: 0.0965 - accuracy: 0.9747 - val_loss: 0.1727 - val_accuracy: 0.9544\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 6s 34ms/step - loss: 0.0894 - accuracy: 0.9766 - val_loss: 0.1768 - val_accuracy: 0.9540\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 5s 28ms/step - loss: 0.0833 - accuracy: 0.9781 - val_loss: 0.1809 - val_accuracy: 0.9536\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 5s 28ms/step - loss: 0.0777 - accuracy: 0.9796 - val_loss: 0.1831 - val_accuracy: 0.9530\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 4s 27ms/step - loss: 0.0725 - accuracy: 0.9810 - val_loss: 0.1874 - val_accuracy: 0.9518\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.0677 - accuracy: 0.9823 - val_loss: 0.1946 - val_accuracy: 0.9513\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.0634 - accuracy: 0.9835 - val_loss: 0.1939 - val_accuracy: 0.9517\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.0595 - accuracy: 0.9846 - val_loss: 0.2019 - val_accuracy: 0.9503\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 4s 26ms/step - loss: 0.0560 - accuracy: 0.9856 - val_loss: 0.2081 - val_accuracy: 0.9498\n",
      "Epoch 1/20\n",
      "166/166 [==============================] - 5s 15ms/step - loss: 1.1226 - accuracy: 0.8040 - val_loss: 0.6773 - val_accuracy: 0.8377\n",
      "Epoch 2/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.5240 - accuracy: 0.8645 - val_loss: 0.3596 - val_accuracy: 0.9136\n",
      "Epoch 3/20\n",
      "166/166 [==============================] - 2s 12ms/step - loss: 0.3108 - accuracy: 0.9230 - val_loss: 0.2463 - val_accuracy: 0.9403\n",
      "Epoch 4/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.2263 - accuracy: 0.9413 - val_loss: 0.1984 - val_accuracy: 0.9480\n",
      "Epoch 5/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.1817 - accuracy: 0.9514 - val_loss: 0.1729 - val_accuracy: 0.9529\n",
      "Epoch 6/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.1528 - accuracy: 0.9581 - val_loss: 0.1587 - val_accuracy: 0.9559\n",
      "Epoch 7/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.1330 - accuracy: 0.9625 - val_loss: 0.1500 - val_accuracy: 0.9577\n",
      "Epoch 8/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.1187 - accuracy: 0.9658 - val_loss: 0.1465 - val_accuracy: 0.9584\n",
      "Epoch 9/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.1078 - accuracy: 0.9685 - val_loss: 0.1444 - val_accuracy: 0.9592\n",
      "Epoch 10/20\n",
      "166/166 [==============================] - 2s 13ms/step - loss: 0.0988 - accuracy: 0.9709 - val_loss: 0.1437 - val_accuracy: 0.9595\n",
      "Epoch 11/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.0912 - accuracy: 0.9729 - val_loss: 0.1441 - val_accuracy: 0.9599\n",
      "Epoch 12/20\n",
      "166/166 [==============================] - 3s 16ms/step - loss: 0.0846 - accuracy: 0.9747 - val_loss: 0.1458 - val_accuracy: 0.9594\n",
      "Epoch 13/20\n",
      "166/166 [==============================] - 3s 17ms/step - loss: 0.0787 - accuracy: 0.9765 - val_loss: 0.1475 - val_accuracy: 0.9592\n",
      "Epoch 14/20\n",
      "166/166 [==============================] - 3s 17ms/step - loss: 0.0733 - accuracy: 0.9782 - val_loss: 0.1502 - val_accuracy: 0.9591\n",
      "Epoch 15/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.0686 - accuracy: 0.9796 - val_loss: 0.1534 - val_accuracy: 0.9590\n",
      "Epoch 16/20\n",
      "166/166 [==============================] - 3s 18ms/step - loss: 0.0641 - accuracy: 0.9811 - val_loss: 0.1575 - val_accuracy: 0.9587\n",
      "Epoch 17/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.0600 - accuracy: 0.9823 - val_loss: 0.1609 - val_accuracy: 0.9582\n",
      "Epoch 18/20\n",
      "166/166 [==============================] - 2s 14ms/step - loss: 0.0563 - accuracy: 0.9835 - val_loss: 0.1646 - val_accuracy: 0.9572\n",
      "Epoch 19/20\n",
      "166/166 [==============================] - 2s 15ms/step - loss: 0.0527 - accuracy: 0.9847 - val_loss: 0.1691 - val_accuracy: 0.9568\n",
      "Epoch 20/20\n",
      "166/166 [==============================] - 3s 15ms/step - loss: 0.0494 - accuracy: 0.9859 - val_loss: 0.1725 - val_accuracy: 0.9565\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(15, 5))\n",
    "input_Dim = len(tokenizer.word_index)+1\n",
    "h_selections = [8, 16, 32]\n",
    "k_selections = [1, 2, 3]\n",
    "# Create k-layer RNN network (non-bidirectional) with h hidden units in each layer):\n",
    "for h in h_selections:\n",
    "    for k in k_selections:\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Embedding(input_dim=input_Dim, output_dim=h, mask_zero=True))\n",
    "        for i in range(k):\n",
    "            model.add(tf.keras.layers.SimpleRNN(h, return_sequences=True))\n",
    "        model.add(tf.keras.layers.Dense(len(le.classes_), activation='softmax'))\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        #ini_loss, ini_acc = model.evaluate(word_val, tag_val, verbose=0)\n",
    "        history = model.fit(word_train, tag_train, epochs=20, batch_size=128, validation_data=(word_val, tag_val))\n",
    "        val_loss = history.history['val_loss']\n",
    "        #val_loss.insert(0, ini_loss)\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        #val_acc.insert(0, ini_acc)\n",
    "        ax1.plot(range(20), val_loss, label=f'{h}_{k}_False')\n",
    "        ax2.plot(range(20), val_acc, label=f'{h}_{k}_False')\n",
    "# Create k-layer bidirectional RNN network with h hidden units in each layer):\n",
    "for h in h_selections:\n",
    "    for k in k_selections:\n",
    "        inputs = tf.keras.layers.Input(shape=(None,))\n",
    "        embedding = tf.keras.layers.Embedding(input_dim=input_Dim, output_dim=h, mask_zero=True)(inputs) \n",
    "        RNN_both = embedding\n",
    "        for k in range(k):\n",
    "            forward_RNN = tf.keras.layers.SimpleRNN(h, return_sequences=True)\n",
    "            backward_RNN = tf.keras.layers.SimpleRNN(h, return_sequences=True, go_backwards=True)\n",
    "            RNN_both = tf.keras.layers.Bidirectional(forward_RNN, backward_layer=backward_RNN)(RNN_both)\n",
    "        concat = tf.keras.layers.Concatenate()([RNN_both, embedding])\n",
    "        output = tf.keras.layers.Dense(len(le.classes_), activation='softmax')(concat)\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "        #ini_loss, ini_acc = model.evaluate(word_val, tag_val, verbose=0)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(word_train, tag_train, epochs=20, batch_size=128, validation_data=(word_val, tag_val))\n",
    "        val_loss = history.history['val_loss']\n",
    "        #val_loss.insert(0, ini_loss)\n",
    "        val_acc = history.history['val_accuracy']\n",
    "        #val_acc.insert(0, ini_acc)\n",
    "        ax1.plot(range(20), val_loss, label=f'{h}_{k}_True')\n",
    "        ax2.plot(range(20), val_acc, label=f'{h}_{k}_True')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy') \n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "ax1.set_title('Loss')\n",
    "ax2.set_title('Accuracy')\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "\n",
    "def make_model(hidden_size, layers, bidir = False):\n",
    "    tf.keras.backend.clear_session()\n",
    "    sentence_input1 = tf.keras.Input(shape=(None,), name='sentence_input')\n",
    "\n",
    "    word_embedding = tf.keras.layers.Embedding(input_dim=vocab_size+1, output_dim=hidden_size, name='word_embedding', mask_zero=True)\n",
    "    masked_embeddinbg = word_embedding(sentence_input1)\n",
    "\n",
    "    h_f = []\n",
    "    h_b = []\n",
    "    # First Forward RNN layer\n",
    "    RNN = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True)\n",
    "    hf = RNN(masked_embeddinbg)\n",
    "    h_f.append(hf)\n",
    "    if bidir:\n",
    "        RNN = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, go_backwards=True)\n",
    "        hb = RNN(masked_embeddinbg)\n",
    "        h_b.append(hb)\n",
    "\n",
    "\n",
    "    # Create forward RNNs\n",
    "    for k in range(1,layers):\n",
    "        RNN_f = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True)\n",
    "        hf = RNN_f(hf)\n",
    "        h_f.append(hf)\n",
    "\n",
    "        if bidir:\n",
    "            RNN_b = tf.keras.layers.SimpleRNN(hidden_size, return_sequences=True, go_backwards=True)\n",
    "            hb = RNN_b(hb)\n",
    "            h_b.append(hb)\n",
    "\n",
    "    if bidir:\n",
    "        if layers==1:\n",
    "            h = tf.keras.layers.concatenate([hf, hb])\n",
    "        else:\n",
    "            h = tf.keras.layers.concatenate(h_f+h_b)\n",
    "    else:\n",
    "        if layers==1:\n",
    "            h = h_f[0]\n",
    "        else:\n",
    "            h = tf.keras.layers.concatenate(h_f)\n",
    "\n",
    "\n",
    "    dense = tf.keras.layers.Dense(le.classes_.shape[0], activation='softmax', )\n",
    "    output = dense(h)\n",
    "\n",
    "    model = tf.keras.Model(inputs=sentence_input1, outputs=output, name='_'.join([str(hidden_size), str(layers),str(bidir)]))\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics = 'accuracy')\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "57bcbe5809de2296bb7b1082e4de808642928c9f1fe31f73bfe7fb1e695a281f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
