{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "## Homework 2: Embeddings\n",
    "\n",
    "**Instructor**: Dr. Pavlos Protopapas<br />\n",
    "**Maximum Score**: 89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## INSTRUCTIONS\n",
    "\n",
    "- This homework is a notebook. Download and work on it on your local machine or work on it in Colab.\n",
    "\n",
    "- This homework should be submitted in a team.\n",
    "\n",
    "- Ensure you and your partners together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit.\n",
    "\n",
    "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
    "\n",
    "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
    "\n",
    "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code.\n",
    "\n",
    "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**.\n",
    "\n",
    "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example:\n",
    "```\n",
    "print(f'The R^2 is {R:.4f}')\n",
    "```\n",
    "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
    "\n",
    "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Names of the people who worked on this homework**\n",
    "#### / Wang Hesong/ Chen Taiyi/ Li Yuepeng/ Mao Yuchen/ Yu Lufei/ Zhong Yixiao "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup Notebook**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verify Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version 2.10.0\n",
      "keras version 2.10.0\n",
      "Eager Execution Enabled: True\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Number of replicas: 1\n",
      "Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "GPU Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "All Physical Devices [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version\", tf.__version__)\n",
    "print(\"keras version\", tf.keras.__version__)\n",
    "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
    "\n",
    "# Get the number of replicas\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
    "\n",
    "devices = tf.config.experimental.get_visible_devices()\n",
    "print(\"Devices:\", devices)\n",
    "print(tf.config.experimental.list_logical_devices('GPU'))\n",
    "\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
    "\n",
    "# Better performance with the tf.data API\n",
    "# Reference: https://www.tensorflow.org/guide/data_performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PART 1 [25 points]: Word2Vec from scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "Word2Vec architecture allows us to get *contextual* representations of word tokens.     \n",
    "\n",
    "There are several methods to build a word embedding. We will focus on the SGNS architecture.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1eyozbhsrzRaKc86SM7LblgzVMZKAW8Pe)\n",
    "\n",
    "In this problem, you are asked to build and analyze a Word2Vec architecture trained on Wikipedia articles.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 1: Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "    \n",
    "**1.1 [5 points] Model Processing**\n",
    "<br />\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "    \n",
    "**1.1.1** - Get the data    \n",
    "\n",
    "- Get the data from the `text8.zip` file.\n",
    "    `text8.zip` is a small, *cleaned* subset of a large corpus of data scraped from Wikipedia pages. More details can be found [here](https://paperswithcode.com/sota/language-modelling-on-text8)\n",
    "    It is usually used to quickly train, or test language models.\n",
    "\n",
    "- Split the data by whitespace and print the first 10 words to check if has been correctly loaded.\n",
    "\n",
    "    **NOTE:** For this part of the homework, all words will be in their lowercase for simplicity of analysis\n",
    "<br />    \n",
    "\n",
    "**1.1.2** - Build the dataset\n",
    "\n",
    "- Write a function that takes the `vocabulary_size` and `corpus` as input, and outputs:\n",
    "    - Tokenized data\n",
    "    - count of each token\n",
    "    - A dictionary that maps words to tokens\n",
    "    - A dictionary that maps tokens to words\n",
    "    You can use the same function used in **Lab 3**, or else you can use [`tf.keras.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to write a similar function.\n",
    "- Print the first 10 tokens and reverse them to words to confirm a match to the initial print above.\n",
    "     \n",
    "  \n",
    "Eg. `corpus[:10] = ['this','is,'an','example',...]`\n",
    "\n",
    "`data[:10] = [44,26,24,16,...]`\n",
    "    \n",
    "`reversed_data =['this','is,'an','example',...]`\n",
    "\n",
    "**NOTE**: Choose a sufficiently large vocabulary size. i.e `vocab_size>= 1000`    \n",
    "<br />\n",
    "    \n",
    "**1.1.3** - Build skipgrams with negative samples\n",
    "- Use the `tf.keras.preprocessing.sequence.skipgrams` function to build positive and negative samples \\\n",
    "    for word2vec training. Follow the documentation on how to make the pairs\n",
    "- You are free to choose your own `window_size`, but we recommend a value of 3.\n",
    "- Print 10 pairs of *center* and *context* words with their associated labels.    \n",
    "    \n",
    "Skip-gram Sampling table\n",
    "A large dataset means a larger vocabulary with a higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
    "\n",
    "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to generate a word-frequency rank-based probabilistic sampling table and pass it to skipgrams function.    \n",
    "<br />\n",
    "    \n",
    "**1.1.4** - What is the difference between using a sampling table and not using a sampling table while building the dataset for skipgrams?\n",
    "<br /><br />\n",
    "    \n",
    "</details>\n",
    "    \n",
    "**1.2 [8 points] Building a Word2Vec model**\n",
    "<br />\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "    \n",
    "Build a word2vec model architecture based on the schematic below.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1fBTpoBoG5RZIPTtdogZt37Bw3oUb7tuT)\n",
    "  \n",
    "    \n",
    "- To do so, you will need:\n",
    "    - `tf.keras.layers.Embedding` layer\n",
    "    - `tf.keras.layers.Dot()`\n",
    "    - `tf.keras.Model()` which is the functional API\n",
    "- You can choose an appropriate embedding dimension\n",
    "- Compile the model using `binary_crossentropy()` function and an appropriate optimizer.\n",
    "- Sufficiently train the model.    \n",
    "- Save model weights using the `model.save_weights()` for analysis of **2.3**. More information on saving your weights [here](https://www.tensorflow.org/tutorials/keras/save_and_load)    \n",
    "<br />\n",
    "\n",
    "</details>\n",
    "    \n",
    "    \n",
    "**1.3 [7 points] Post-training analysis**\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "    \n",
    "This segment involves some simple analysis of your trained embeddings.\n",
    "<br /><br />\n",
    "    \n",
    "    \n",
    "**1.3.1** - Vector Algebra on Embeddings\n",
    "\n",
    "Assuming you have chosen a sufficiently large `vocab_size`, find the embeddings for:\n",
    "    \n",
    "1. King\n",
    "2. Male\n",
    "3. Female\n",
    "4. Queen\n",
    "    \n",
    "Find the vector `v = King - Male + Female` and find its `cosine_similarity()` with the embedding for 'Queen'.\n",
    "You can use the `cosine_similarity()` function defined in the session 3 exercise.\n",
    "\n",
    "**NOTE**:The `cosine_similarity()` value, must be greater than `0.9`; If it is not, this implies that your word2vec embeddings are not well-trained.\n",
    "\n",
    "Write a function `most_similar()`, which finds the top-n words most similar to the given word. Use this function to find the words most similar to `king`.\n",
    "    \n",
    "**Conceptual Question** Why can't we use `cosine_similarity()` as a `loss_function`?\n",
    "    \n",
    "<br />\n",
    "    \n",
    "**1.3.2** - Visualizing Embeddings\n",
    "\n",
    "Find the embeddings for the words:\n",
    "1. 'January'\n",
    "2. 'February'\n",
    "3. 'March'\n",
    "4. 'April'\n",
    "    \n",
    "Find the `cosine_similarity()` of 'january' with each of 'february`, 'march', 'april' (which should be high values).\n",
    "    \n",
    "Save your trained weights. Recreate the network you have created above and initialize it with random weights. Compute the `cosine_similarity()` values. The values should be small (because the embeddings are random).\n",
    "    \n",
    "Use a demonstrative plot to show the `before & after` of the 4 embeddings. Here are some suggestions:\n",
    "    1. PCA/TSNE for dimensionality reduction\n",
    "    2. Radar plot to show all embedding dimensions\n",
    "    \n",
    "Bonus points for using creative means to demonstrate how the embeddings change after training.\n",
    "\n",
    "Here is a [video](https://youtu.be/VDl_iA8m8u0) of a sample demonstration. We used a custom callback to get embeddings during training.  \n",
    "        \n",
    "\n",
    "<br />\n",
    "    \n",
    "**1.3.3** - Embedding and Context Matrix\n",
    "    \n",
    "    \n",
    "**1.3.3.1** Investigate the relation between the Embedding & Context matrix. Again use the `cosine_similarity()` function to find the average value across all the words in the embedding and context matrix, i.e:\n",
    "  - For the word 'dog', find the embedding value, and context value. <br>\n",
    "  - Calculate the `cosine_similarity()` between the two <br>\n",
    "  - Repeat the same for every word in the vocabulary and calculate the average value of the `cosine_similarity()`\n",
    "<br />\n",
    "\n",
    "**1.3.3.2** Answer the following question and explain:\n",
    "\n",
    "**Question:** The embedding and context matrices should be identical.\n",
    "<br /><br />\n",
    "    \n",
    " </details>\n",
    "\n",
    "**1.4 [5 points] Learning phrases**\n",
    "    \n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "As per the original paper by [Mikolov et al](https://arxiv.org/abs/1301.3781) many phrases have a meaning that is not a simple composition of the meanings of their individual words.\n",
    "For eg. `new york` is one entity, however, as per our analysis above, we have two separate entities `new` & `york` which can have different meanings independently.    \n",
    "To learn vector representation for phrases, we first find words that\n",
    "appear frequently together, and infrequently in other contexts.\n",
    "    \n",
    "As per the analysis in the paper, we can use a formula to rank commonly used word pairs, and take the first 100 commonly occurring pairs.\n",
    "$$\\operatorname{score}\\left(w_{i}, w_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i} w_{j}\\right)-\\delta}{\\operatorname{count}\\left(w_{i}\\right) \\times \\operatorname{count}\\left(w_{j}\\right)}$$\n",
    "\n",
    "**NOTE:** For simplicity of analysis, we take the discounting factor $\\delta$ as 0, and take bi-gram combinations. You can experiment with tri-grams for word pairs such as `New_York_Times`.     \n",
    "<br /><br />\n",
    "\n",
    "    \n",
    "**1.4.1** - Find 100 most common bi-grams\n",
    "\n",
    "From the tokenized data above, find the count for each bigram pair.\n",
    "    \n",
    "For each such pair, find the score associated with each token pair using the formula above.\n",
    "    \n",
    " Pick the top 100 pairs based on the score. (Higher the better). To understand the `score()` function we suggest you read the paper mentioned above.\n",
    "    \n",
    "Replace the original `text8` file with the pairs as one entity. E.g., if `prime, minister` is a commonly occurring pair, replace `... prime minister ...' in the original corpus with a single entity `prime_minister`. Do this for all 100 pairs.\n",
    "<br /><br />\n",
    "    \n",
    "**1.4.2** - Retrain word2vec\n",
    "With the new corpus generated as above, build the dataset, use skipgrams and retrain your word2vec with a sufficiently large vocabulary.\n",
    "\n",
    "Write a function `most_dissimilar()`, similar to the `most_similar()` function, however this finds the top-n words which are **most dissimilar** to the given word.\n",
    "Use this function defined above to find the entities most dissimilar to `united_kingdom`\n",
    "    \n",
    "Compare the above with separate tokens for `united` & `kingdom` and the sum of the vectors (to get this, you may need a sufficiently large vocabulary (>2000)).\n",
    "<br /> <br />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 1: Solutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "#### **1.1 [5 points] Model processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1.1** - Get the data  \n",
    "\n",
    "- Get the data from the `text8.zip` file.\n",
    "    `text8.zip` is a small, *cleaned* subset of a large corpus of data scraped from Wikipedia pages. More details can be found [here](https://paperswithcode.com/sota/language-modelling-on-text8)\n",
    "    It is usually used to quickly train, or test language models.\n",
    "    \n",
    "- Split the data by whitespace and print the first 10 words to check if has been correctly loaded.\n",
    "\n",
    "    **NOTE:** For this part of the homework, all words will be in their lowercase for simplicity of analysis   \n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code to read the data\n",
    "\n",
    "# Download\n",
    "urllib.request.urlretrieve(\"https://github.com/dlops-io/datasets/releases/download/v1.0/text8.zip\", \"text8.zip\")\n",
    "\n",
    "# Unzip and read data\n",
    "filename = 'text8.zip'\n",
    "with zipfile.ZipFile(filename) as f:\n",
    "    vocabulary = tf.compat.as_str(f.read(f.namelist()[0])).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**1.1.2** - Build the dataset  \n",
    "\n",
    "- Write a function that takes the `vocabulary_size` and `corpus` as input, and outputs:\n",
    "    - Tokenized data\n",
    "    - count of each token\n",
    "    - A dictionary that maps words to tokens\n",
    "    - A dictionary that maps tokens to words\n",
    "    You can use the same function used in **Lab 3**, or else you can use [`tf.keras.Tokenizer`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) to write a similar function.\n",
    "- Print the first 10 tokens and reverse them to words to confirm a match to the initial print above.\n",
    "     \n",
    "  \n",
    "Eg. `corpus[:10] = ['this','is,'an','example',...]`\n",
    "\n",
    "`data[:10] = [44,26,24,16,...]`\n",
    "    \n",
    "`reversed_data =['this','is,'an','example',...]`\n",
    "\n",
    "**NOTE**: Choose a sufficiently large vocabulary size. i.e `vocab_size>= 1000`\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "\n",
    "   # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocab_size)\n",
    "del vocabulary  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**1.1.3** - Build skipgrams with negative samples\n",
    "- Use the `tf.keras.preprocessing.sequence.skipgrams` function to build positive and negative samples \\\n",
    "    for word2vec training. Follow the documentation on how to make the pairs\n",
    "- You are free to choose your own `window_size`, but we recommend a value of 3.\n",
    "- Print 10 pairs of *center* and *context* words with their associated labels.    \n",
    "    \n",
    "Skip-gram Sampling table\n",
    "A large dataset means a larger vocabulary with a higher number of more frequent words such as stopwords. Training examples obtained from sampling commonly occurring words (such as the, is, on) don't add much useful information for the model to learn from. [Mikolov et al.](https://papers.nips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf) suggest subsampling of frequent words as a helpful practice to improve embedding quality.\n",
    "\n",
    "The `tf.keras.preprocessing.sequence.skipgrams` function accepts a sampling table argument to encode probabilities of sampling any token. You can use the `tf.keras.preprocessing.sequence.make_sampling_table` to generate a word-frequency rank-based probabilistic sampling table and pass it to skipgrams function.    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**1.1.4** - What is the difference between using a sampling table and not using a sampling table while building the dataset for skipgrams?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "#### **1.2 [8 points]** **Building a word2vec model**\n",
    "\n",
    "Build a word2vec model architecture based on the schematic below.\n",
    "\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1fBTpoBoG5RZIPTtdogZt37Bw3oUb7tuT)\n",
    "  \n",
    "    \n",
    "- To do so, you will need:\n",
    "    - `tf.keras.layers.Embedding` layer\n",
    "    - `tf.keras.layers.Dot()`\n",
    "    - `tf.keras.Model()` which is the functional API\n",
    "- You can choose an appropriate embedding dimension\n",
    "- Compile the model using `binary_crossentropy()` function and an appropriate optimizer.\n",
    "- Sufficiently train the model.    \n",
    "- Save model weights using the `model.save_weights()` for analysis of **2.3**. More information on saving your weights [here](https://www.tensorflow.org/tutorials/keras/save_and_load)    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "#### **1.3 [7 points] Post-training analysis**\n",
    "    \n",
    "This segment involves some simple analysis of your trained embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "**1.3.1** - Vector Algebra on Embeddings\n",
    "\n",
    "Assuming you have chosen a sufficiently large `vocab_size`, find the embeddings for:\n",
    "    \n",
    "1. King\n",
    "2. Male\n",
    "3. Female\n",
    "4. Queen\n",
    "    \n",
    "Find the vector `v = King - Male + Female` and find its `cosine_similarity()` with the embedding for 'Queen'.\n",
    "You can use the `cosine_similarity()` function defined in the session 3 exercise.\n",
    "\n",
    "**NOTE**:The `cosine_similarity()` value, must be greater than `0.9`; If it is not, this implies that your word2vec embeddings are not well-trained.\n",
    "\n",
    "Write a function `most_similar()`, which finds the top-n words most similar to the given word. Use this function to find the words most similar to `king`.\n",
    "    <br />\n",
    "    <br />\n",
    "    \n",
    "**Conceptual Question** Why can't we use `cosine_similarity()` as a `loss_function`?\n",
    "    \n",
    "<br />\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "    \n",
    "**1.3.2** - Visualizing Embeddings\n",
    "\n",
    "Find the embeddings for the words:\n",
    "1. 'January'\n",
    "2. 'February'\n",
    "3. 'March'\n",
    "4. 'April'\n",
    "    \n",
    "Find the `cosine_similarity()` of 'january' with each of 'february`, 'march', 'april' (which should be high values).\n",
    "    \n",
    "Save your trained weights. Recreate the network you have created above and initialize it with random weights. Compute the `cosine_similarity()` values. The values should be small (because the embeddings are random).\n",
    "    \n",
    "Use a demonstrative plot to show the `before & after` of the 4 embeddings. Here are some suggestions:\n",
    "    1. PCA/TSNE for dimensionality reduction\n",
    "    2. Radar plot to show all embedding dimensions\n",
    "    \n",
    "Bonus points for using creative means to demonstrate how the embeddings change after training.\n",
    "\n",
    "Here is a [video](https://youtu.be/VDl_iA8m8u0) of a sample demonstration. We used a custom callback to get embeddings during training.  \n",
    "            \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "#### **1.4 [5 points] Learning phrases**\n",
    "    \n",
    "As per the original paper by [Mikolov et al](https://arxiv.org/abs/1301.3781) many phrases have a meaning that is not a simple composition of the meanings of their individual words.\n",
    "For eg. `new york` is one entity, however, as per our analysis above, we have two separate entities `new` & `york` which can have different meanings independently.    \n",
    "To learn vector representation for phrases, we first find words that\n",
    "appear frequently together, and infrequently in other contexts.\n",
    "    \n",
    "As per the analysis in the paper, we can use a formula to rank commonly used word pairs, and take the first 100 commonly occurring pairs.\n",
    "$$\\operatorname{score}\\left(w_{i}, w_{j}\\right)=\\frac{\\operatorname{count}\\left(w_{i} w_{j}\\right)-\\delta}{\\operatorname{count}\\left(w_{i}\\right) \\times \\operatorname{count}\\left(w_{j}\\right)}$$\n",
    "\n",
    "**NOTE:** For simplicity of analysis, we take the discounting factor $\\delta$ as 0, and take bi-gram combinations. You can experiment with tri-grams for word pairs such as `New_York_Times`.     \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4.1** - Find 100 most common bi-grams\n",
    "\n",
    "From the tokenized data above, find the count for each bigram pair.\n",
    "    \n",
    "For each such pair, find the score associated with each token pair using the formula above.\n",
    "    \n",
    " Pick the top 100 pairs based on the score. (Higher the better). To understand the `score()` function we suggest you read the paper mentioned above.\n",
    "    \n",
    "Replace the original `text8` file with the pairs as one entity. E.g., if `prime, minister` is a commonly occurring pair, replace `... prime minister ...' in the original corpus with a single entity `prime_minister`. Do this for all 100 pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training data again\n",
    "filename = 'text8.zip'\n",
    "with zipfile.ZipFile(filename) as f:\n",
    "# Read the data into a list of strings.\n",
    "    super_text = tf.compat.as_str(f.read(f.namelist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in to complete this function\n",
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "\n",
    "    # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use lower case and split as before\n",
    "corpus = super_text.lower().split()\n",
    "data, count, dictionary, reversed_dictionary = build_dataset(corpus,6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**1.4.2** - Retrain word2vec   \n",
    "With the new corpus generated as above, build the dataset, use skipgrams and retrain your word2vec with a sufficiently large vocabulary.\n",
    "\n",
    "Write a function `most_dissimilar()`, similar to the `most_similar()` function, however this finds the top-n words which are **most dissimilar** to the given word.\n",
    "Use this function defined above to find the entities most dissimilar to `united_kingdom`\n",
    "    \n",
    "Compare the above with separate tokens for `united` & `kingdom` and the sum of the vectors (to get this, you may need a sufficiently large vocabulary (>2000).\n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PART 2 [64 points]: IMDB Sentiment Analysis using ELMo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "   \n",
    "Sentiment analysis, also known as opinion mining or emotion AI, is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information.\n",
    "    \n",
    "For this part of the homework, we will be using the IMDB dataset, which is publicly available [here](http://ai.stanford.edu/~amaas/data/sentiment/).    \n",
    "\n",
    "This represents a \"many-to-one\" problem, with the output classified as a `positive` or `negative` sentiment, depending on the words used in the review.\n",
    "    \n",
    "    \n",
    "In the first part of this section, you are expected to build a language model to train a basic ELMo.\n",
    "    \n",
    "Although the original ELMo implementation uses *Character Embeddings*, for the sake of this homework, we will use word embeddings instead.\n",
    "    \n",
    "Read more about the ELMo paper [here](https://arxiv.org/pdf/1802.05365.pdf).\n",
    "\n",
    "In the second part of this subsection, you will use the generated ELMo embeddings in a deep-learning model to perform sentiment analysis using the IMDB dataset.\n",
    "    \n",
    "You will compare its performance, with a baseline model without any trained embeddings, and another model which directly uses the `word2vec` embeddings.\n",
    "\n",
    "<br />\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 2: Questions**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    " **2.1 [15 points] Preprocess the dataset**\n",
    "<br />\n",
    "\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "    \n",
    "**2.1.1** - Load the dataset\n",
    "\n",
    "For simplicity, we will use the training split of the IMDB dataset.\n",
    "- Limit to the most frequent 5000 words.\n",
    "- Do not skip any frequently occurring words.\n",
    "- Limit the largest review to a maximum of 200 words only.\n",
    "    \n",
    "    \n",
    "**NOTE**: You can use the `imdb.get_word_index()` to get the mapping between tokens and words. This will load a dictionary with the mappings, which have to be corrected. A helper code is provided below to fix the dictionary.\n",
    "    \n",
    "Read more about `tf.keras.datasets.imdb` [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data?version=nightly).    \n",
    "    \n",
    "To each review, you must add an end-of-sentence token \\<\\/s>.\n",
    "\n",
    "Eg. Review: \"\\<s\\> This movie is so bad, I had to leave early\"\n",
    "    \n",
    "Modified review: \"\\<s\\> This movie is so bad, I had to leave early \\<\\/s\\>\"\n",
    "<br />\n",
    "\n",
    "**2.1.2** - Load the `word2vec` embeddings\n",
    "    \n",
    "You will use the pre-trained `word2vec` embeddings for this section of the homework. This file can be downloaded from [here](https://github.com/dlops-io/datasets/releases/download/v1.0/GoogleNews-vectors-negative300.bin.gz) and can be accessed like a dictionary.\n",
    "    \n",
    "**NOTE**: To access the pre-trained embeddings, [gensim](https://pypi.org/project/gensim/) library can be used.\n",
    "\n",
    "Check if the embeddings for the start token (\\<s\\>) and end token (\\</s\\>) are present in the loaded `word2vec` embeddings.\n",
    "\n",
    "Add the \\<s\\> and\\</s\\> tokens to the `word2vec` embeddings as random vectors if they are not present.\n",
    "\n",
    "Create an `embedding_matrix` that will consist of the words present in word2vec. It will be a matrix of dimension `num_words X embedding_dim` `(5000 X 300)` including the addition of start and end tokens.\n",
    "<br /><br />\n",
    "**2.1.3** Prepare data for model training:\n",
    "\n",
    "- Not all the words in the reviews are present in the embeddings file. Hence, if it is not present, you must OMIT that word from the sentence.\n",
    "   \n",
    "E.g. If `and` token is not present in the embeddings:\n",
    "   \n",
    "```\n",
    "OLD SENTENCE: <s>The movie was good and I really liked it </s>\n",
    "    \n",
    "NEW SENTENCE: <s> The movie was good I really liked it </s>\n",
    "    \n",
    "```\n",
    "\n",
    "- Split the data (`x_train` (tokens list), `y_train` (class list)) into 80% training and 20% validation. We will use the `y_train` (which is the sentiment associated with each movie review) only in Part 2.3 for sentiment analysis.\n",
    "\n",
    "</details>\n",
    "<br />\n",
    "    \n",
    "**2.2 [34 points] Define and train the model**\n",
    "    \n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "    \n",
    "We define an *ELMo-like* language model using bi-directional LSTMs and residual connections  **without** the character CNN to simplify the analysis.\n",
    "    \n",
    "We will use the `word2vec` embeddings instead of the character representations of the CNN.\n",
    "    \n",
    "For simplicity, we train our *ELMo-like* language model on the IMDB dataset itself. But generally, language models are trained on much larger corpora.  \n",
    "\n",
    "**2.2.1** Build a `tf.data.Dataset` for training, and another one for validation\n",
    "\n",
    "- Set an appropriate batch size (32, 64, 128, 256,...) This value will be determined by the GPU you have.\n",
    "\n",
    "- Set `train_shuffle_buffer_size` and `validation_shuffle_buffer_size` to the length of training data and validation data respectively\n",
    "\n",
    "- `tf.data.Dataset` is an efficient way to build data pipelines. Instead of preprocessing the entire dataset, we can preprocess a batch. It is faster and consumes fewer resources, which is optimal for training.\n",
    "\n",
    "- Hint: When creating tf.data.Dataset use `tf.ragged.constant` to convert your ragged tokens to ragged tensors\n",
    "\n",
    "- `dataset.map` enable us to apply a function to each element of the batch individually. The parameter `num_parallel_calls` allow us to control how many threads we will use to feed the network. It can be set to `num_parallel_calls=AUTOTUNE`. You will use the `transform_pad` function to perform model-specific data processing.\n",
    "\n",
    "- When building the tf.data pipeline use the following order:\n",
    "  - shuffle\n",
    "  - batch\n",
    "  - map\n",
    "  - prefetch\n",
    "<br>\n",
    "<br>\n",
    "- After you build your train and validation dataset, use `dataset.take(1)` to view the first row of data from the training dataset. It is important to verify the data input and output dimensions before modelling\n",
    "<br /><br />\n",
    "    \n",
    "**2.2.2** Building the language model\n",
    "    \n",
    "*Transform Input within the model:*\n",
    "\n",
    "*   In forward LSTM, we use the n-th token to predict the (n+1)-th token. Hence we want to discard the last one, the end of sentence token, `</s>`, from all the sentences. Remember all the sentences are padded, so the `</s>` will not be the last element of the sequence.\n",
    "\n",
    "   * One way of achieving that is, using a Boolean mask with help of [tf.sequence_mask](https://www.tensorflow.org/api_docs/python/tf/sequence_mask) or with the help of [tf.gather_nd](https://www.tensorflow.org/api_docs/python/tf/gather_nd), which can be used to select specific elements from a tensor based on their indices. Remember you can combine multiple boolean masks via multiple logic operations. We also encourage you to come up with your solutions.\n",
    "   * Note that after using boolean masks your outputs will be flattened out and you have to reshape them back with the appropriate batch size. Remember that you removed the end of sentence token, hence the length of the sequence would be length-1.\n",
    "\n",
    "    \n",
    "![](https://drive.google.com/uc?export=view&id=1f5bPplDGlRUdfii5X1bD2kd20SKRyOSo)\n",
    "  \n",
    "\n",
    "* For the backward LSTM layers, the next word prediction task is doing the reverse of what it was doing for the forward LSTM layers. We aim to predict the n-th token with the (n+1)-th token. To achieve this we remove the start of the sentence token `<s>` from all the sentences.\n",
    "\n",
    "\n",
    "The model's inputs should be followed by a `tf.keras.layers.Embedding()` layer. The `Embedding` layer will act as a lookup table to convert token inputs to their corresponding word2vec values. When initialising the `Embedding` layer, set the layer weights with the `embedding_matrix` you had built in the previous question.\n",
    "\n",
    "Set the trainable to false and mask_zero to true in the `Embedding` layer. The input dimension should be the number of rows in `embedding_matrix` and the output dimension should be your embedding dimension of `300`.\n",
    "    \n",
    "Refer to this image from the lecture slides on ELMo.\n",
    "    \n",
    "    \n",
    "![](https://drive.google.com/uc?export=view&id=1fNPnrBR7Wfh_Jci70QaHde1L3nTcs9-D)\n",
    "    \n",
    "*go_backwards in backward LSTM layers*\n",
    "\n",
    "* To predict the words backwards, we will use the go_backwards parameter present in the TensorFlow LSTM layer implementation. Remember to reverse the output of each backward LSTM layer before using it. Refer to the documentation of [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Remember to invert the sequence dimension and not the embedding dimension.\n",
    "\n",
    "Remember to use the **same** *softmax* layer on both the forward and the backward LSTMs.\n",
    "    \n",
    "This will give you an `output_f` and `output_b` which you will evaluate with your two targets.  \n",
    "\n",
    "$$L = -\\sum y \\cdot log(\\widehat{y}_{right}) -\\sum y \\cdot log(\\widehat{y}_{left})$$\n",
    "\n",
    "Use an appropriate loss function, and optimizer and train the network sufficiently.   \n",
    "\n",
    "Finally, plot the training history of the train and validation loss.\n",
    "<br />\n",
    "    \n",
    "**NOTE**: Use native tensorflow functions like `tf.shape` instead of numpy functions like `np.shape`.\n",
    "<br />  \n",
    "\n",
    "**2.2.3** Extracting ELMo embeddings\n",
    "    \n",
    "Use the Functional API to build another model called `Toy_ELMo` to obtain the embeddings.\n",
    "\n",
    "*To concatenate the forward and backward LSTM hidden states we have to align them, which will be achieved by removing the start and end of sentence token embeddings from the forward and the backwards LSTM hidden states respectively. You can use the same logic that you used to process the input. It is necessary to reverse the output of the backward LSTM layers before using it in the Toy ELMo model.*\n",
    "\n",
    "The obtained embeddings should be:\n",
    "    \n",
    "1. The `word2vec` embeddings\n",
    "        \n",
    "    This is just the output after the masked layer in the language model defined above.    \n",
    "    \n",
    "2. The first bidirectional-LSTM layer embeddings\n",
    "\n",
    "    This will be the concatenation of the first LSTM layers of the language model (`lstm1 forward + lstm1 backwards`).\n",
    "    \n",
    "3. The second bidirectional-LSTM layer embeddings\n",
    "\n",
    "    This will be the concatenation of the second LSTM layer of the language model (`lstm2 forward + lstm2 backwards`).    \n",
    "    \n",
    "- Make a test prediction of your `Toy_ELMo` model  \n",
    "\n",
    "</details>\n",
    "    \n",
    "<br />\n",
    "    \n",
    " **2.3 [15 points] Transfer Learning**\n",
    "    \n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Click for instructions</b>\n",
    "</font>\n",
    "</summary>\n",
    "    \n",
    "Once you've sufficiently trained your ELMo embeddings, we can use it for a downstream task such as sentiment analysis.\n",
    "    \n",
    "**2.3.1** - Baseline model:\n",
    "    \n",
    "For the baseline model, you will use:\n",
    "\n",
    "- `tf.keras.Layers.Embedding()` layer\n",
    "-  2 layers of `GRU` with `hidden_size=300`\n",
    "-  Dense output layer\n",
    "    \n",
    "You will build a `tf.data.Dataset` similar to the one created in Section 2.2.1 but instead of having a target as a series of tokens, the target should only be a class (positive or negative sentiment). Unlike in 2.2.1 we only need a single sequence of tokens for this model\n",
    "\n",
    "Train it for sufficient epochs using an appropriate optimizer and learning rate.\n",
    "\n",
    "**2.3.2** - Directly using pre-trained `word2vec`:\n",
    "    \n",
    "For this section, use the pre-trained `word2vec` embeddings directly into your model.\n",
    "\n",
    "You can use the same `tf.data.Dataset` from 2.3.1 for this model\n",
    "    \n",
    "Train, and compare its performance with the baseline model defined above **using the same architecture** as above.\n",
    "    <br /><br />\n",
    "**2.3.3** - You have already done sentiment analysis using `tf.keras.layers.Embedding()`. You will now aim to beat that baseline using your ELMo embeddings.  \n",
    "\n",
    "Using ELMo embeddings:\n",
    "\n",
    "You will build another `tf.data.Dataset` similar to the one created in Section 2.2.1 but instead of having a target as a series of tokens, the target should only be a class (positive or negative sentiment). This model also requires two inputs one for forward and one for backward LSTM.   \n",
    "\n",
    "For this model, you will use:\n",
    "- `Toy_ELMo` model after the input layer\n",
    "-  Sauce layer\n",
    "-  2 layers of `GRU` with `hidden_size=300`\n",
    "-  Dense output layer\n",
    "    \n",
    "**NOTE**: Set `Toy_ELMo.trainable` to `False` to avoid retraining the model.\n",
    "        \n",
    "Create the **sauce** layer to combine the three embeddings from your `Toy_ELMo`. You should have **three** trainable parameters in this layer\n",
    "\n",
    "$$ELMo_{t} = \\gamma \\sum_{j=0}^{L} s_{j} h_{t}^{j}$$\n",
    "    \n",
    "Since we are not using any other embeddings, we will set the value of $\\gamma$ to 1. <br>\n",
    "Train the modified model sufficiently, and compare it to the previously trained models.\n",
    "</details>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PART 2: Solutions**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "\n",
    "\n",
    "#### **2.1 [15 points] Preprocess the dataset**\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1.1** - Load the dataset\n",
    "\n",
    "For simplicity, we will use the training split of the IMDB dataset.\n",
    "- Limit to the most frequent 5000 words.\n",
    "- Do not skip any frequently occurring words.\n",
    "- Limit the largest review to a maximum of 200 words only.\n",
    "    \n",
    "    \n",
    "**NOTE**: You can use the `imdb.get_word_index()` to get the mapping between tokens and words. This will load a dictionary with the mappings, which have to be corrected. A helper code is provided below to fix the dictionary.\n",
    "    \n",
    "Read more about `tf.keras.datasets.imdb` [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data?version=nightly).    \n",
    "    \n",
    "To each review, you must add an end-of-sentence token \\<\\/s>.\n",
    "\n",
    "Eg. Review: \"\\<s\\> This movie is so bad, I had to leave early\"\n",
    "    \n",
    "Modified review: \"\\<s\\> This movie is so bad, I had to leave early \\<\\/s\\>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "### Helper code to fix the mapping of the imdb word index\n",
    "\n",
    "index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# we need to add 3 from the indices because 0 is 'padding', 1 is 'start of sequence' and 2 is 'unknown'\n",
    "\n",
    "inv_index = {j+3:i for i,j in index.items()}\n",
    "\n",
    "# Tags for start and end of sentence\n",
    "\n",
    "inv_index[1] = '<s>'\n",
    "inv_index[2] = 'UNK'\n",
    "inv_index[3] = '</s>'\n",
    "\n",
    "index = {j:i for i,j in inv_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "**2.1.2** - Load the `word2vec` embeddings\n",
    "    \n",
    "You will use the pre-trained `word2vec` embeddings for this section of the homework. This file can be downloaded from [here](https://github.com/dlops-io/datasets/releases/download/v1.0/GoogleNews-vectors-negative300.bin.gz) and can be accessed like a dictionary.\n",
    "    \n",
    "**NOTE**: To access the pre-trained embeddings, [gensim](https://pypi.org/project/gensim/) library can be used.\n",
    "\n",
    "Check if the embeddings for the start token (\\<s\\>) and end token (\\</s\\>) are present in the loaded `word2vec` embeddings.\n",
    "\n",
    "Add the \\<s\\> and\\</s\\> tokens to the `word2vec` embeddings as random vectors if they are not present.\n",
    "\n",
    "Create an `embedding_matrix` that will consist of the words present in word2vec. It will be a matrix of dimension `num_words X embedding_dim` `(5000 X 300)` including the addition of start and end tokens.\n",
    "<br />\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Word2Vec embeddings using gensim library\n",
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "        \n",
    "    \n",
    "**2.1.3** - Prepare data for model training:\n",
    "\n",
    "- Not all the words in the reviews are present in the embeddings file. Hence, if it is not present, you must OMIT that word from the sentence.\n",
    "   \n",
    "E.g. If `and` token is not present in the embeddings:\n",
    "   \n",
    "```\n",
    "OLD SENTENCE: <s>The movie was good and I really liked it </s>\n",
    "    \n",
    "NEW SENTENCE: <s> The movie was good I really liked it </s>\n",
    "    \n",
    "```\n",
    "\n",
    "- Split the data (`x_train` (tokens list), `y_train` (class list)) into 80% training and 20% validation. We will use the `y_train` (which is the sentiment associated with each movie review) only in Part 2.3 for sentiment analysis.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "#### **2.2 [34 points] Define and train the model**\n",
    "    \n",
    "We define an *ELMo-like* language model using bi-directional LSTMs and residual connections  **without** the character CNN to simplify the analysis.\n",
    "    \n",
    "We will use the `word2vec` embeddings instead of the character representations of the CNN.\n",
    "    \n",
    "For simplicity, we train our *ELMo-like* language model on the IMDB dataset itself. But generally, language models are trained on much larger corpora.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2.1** - Build a `tf.data.Dataset` for training, and another one for validation\n",
    "\n",
    "- Set an appropriate batch size (32, 64, 128, 256,...) This value will be determined by the GPU you have.\n",
    "\n",
    "- Set `train_shuffle_buffer_size` and `validation_shuffle_buffer_size` to the length of training data and validation data respectively\n",
    "\n",
    "- `tf.data.Dataset` is an efficient way to build data pipelines. Instead of preprocessing the entire dataset, we can preprocess a batch. It is faster and consumes fewer resources, which is optimal for training.\n",
    "\n",
    "- Hint: When creating tf.data.Dataset use `tf.ragged.constant` to convert your ragged tokens to ragged tensors\n",
    "\n",
    "- `dataset.map` enable us to apply a function to each element of the batch individually. The parameter `num_parallel_calls` allow us to control how many threads we will use to feed the network. It can be set to `num_parallel_calls=AUTOTUNE`. You will use the `transform_pad` function to perform model-specific data processing.\n",
    "\n",
    "- When building the tf.data pipeline use the following order:\n",
    "  - shuffle\n",
    "  - batch\n",
    "  - map\n",
    "  - prefetch\n",
    "<br><br>\n",
    "- After you build your train and validation dataset, use `dataset.take(1)` to view the first row of data from the training dataset. It is important to verify the data input and output dimensions before modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Code\n",
    "batch_size = 64\n",
    "train_shuffle_buffer_size = len(tokens_list_train)\n",
    "validation_shuffle_buffer_size = len(tokens_list_val)\n",
    "\n",
    "# Fill the required cells to complete the function\n",
    "def transform_pad(input, output,n):\n",
    "\n",
    "    # You will  transform the input at the run time\n",
    "    input = input.to_tensor(default_value=0, shape=[None, None])\n",
    "\n",
    "    # Transform the output for the f and b LSTM\n",
    "    output_f = output[:,1:]\n",
    "    output_b = output[:,:-1]\n",
    "\n",
    "\n",
    "    # Pad the outputs\n",
    "    output_f = output_f.to_tensor(default_value=0, shape=[None, None])\n",
    "    output_b = output_b.to_tensor(default_value=0, shape=[None, None])\n",
    "\n",
    "    return (input,n),(output_f, output_b)\n",
    "\n",
    "# Calculate and store length of each sentence\n",
    "N_train = [len(n) for n in tokens_list_train]\n",
    "N_val = [len(n) for n in tokens_list_val]\n",
    "\n",
    "N_train = tf.constant(N_train, tf.int32)\n",
    "N_val = tf.constant(N_val, tf.int32)\n",
    "\n",
    "# Use tensorflow ragged constants to get the ragged version of data\n",
    "train_processed_x = tf.ragged.constant(tokens_list_train)\n",
    "validate_processed_x = tf.ragged.constant(tokens_list_val)\n",
    "train_processed_y = tf.ragged.constant(tokens_list_train)\n",
    "validate_processed_y = tf.ragged.constant(tokens_list_val)\n",
    "\n",
    "# Create TF Dataset\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_processed_x, train_processed_y,N_train))\n",
    "validation_data = tf.data.Dataset.from_tensor_slices((validate_processed_x, validate_processed_y,N_val))\n",
    "\n",
    "#############\n",
    "# Train data\n",
    "#############\n",
    "# Apply all data processing logic\n",
    "train_data = train_data.shuffle(buffer_size=train_shuffle_buffer_size)\n",
    "train_data = train_data.batch(batch_size)\n",
    "train_data = train_data.map(transform_pad, num_parallel_calls=AUTOTUNE)\n",
    "train_data = train_data.prefetch(AUTOTUNE)\n",
    "\n",
    "##################\n",
    "# Validation data\n",
    "##################\n",
    "# Apply all data processing logic\n",
    "#validation_data = validation_data.shuffle(buffer_size=validation_shuffle_buffer_size)\n",
    "validation_data = validation_data.batch(batch_size)\n",
    "validation_data = validation_data.map(transform_pad, num_parallel_calls=AUTOTUNE)\n",
    "validation_data = validation_data.prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"train_data\",train_data)\n",
    "print(\"validation_data\",validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some data from tf dataset\n",
    "for (input_f,n) ,(output_f, output_b) in train_data.take(1):\n",
    "  print(input_f.shape)\n",
    "  print(input_f[0])\n",
    "  print(n.shape)\n",
    "  print(n[0])\n",
    "  print(\"************************\")\n",
    "  print(output_f.shape,output_b.shape)\n",
    "  print(output_f[0])\n",
    "  print(output_b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "**2.2.2** - Building the language model\n",
    "    \n",
    "*Transform Input within the model:*\n",
    "\n",
    "*   In forward LSTM, we use the n-th token to predict the (n+1)-th token. Hence we want to discard the last one, the end of sentence token, `</s>`, from all the sentences. Remember all the sentences are padded, so the `</s>` will not be the last element of the sequence.\n",
    "\n",
    "   * One way of achieving that is, using a Boolean mask with help of [tf.sequence_mask](https://www.tensorflow.org/api_docs/python/tf/sequence_mask) or with the help of [tf.gather_nd](https://www.tensorflow.org/api_docs/python/tf/gather_nd), which can be used to select specific elements from a tensor based on their indices. Remember you can combine multiple boolean masks via multiple logic operations. We also encourage you to come up with your solutions.\n",
    "   * Note that after using boolean masks your outputs will be flattened out and you have to reshape them back with the appropriate batch size. Remember that you removed the end of sentence token, hence the length of the sequence would be length-1.\n",
    "\n",
    "    \n",
    "![](https://drive.google.com/uc?export=view&id=1f5bPplDGlRUdfii5X1bD2kd20SKRyOSo)\n",
    "  \n",
    "\n",
    "* For the backward LSTM layers, the next word prediction task is doing the reverse of what it was doing for the forward LSTM layers. We aim to predict the n-th token with the (n+1)-th token. To achieve this we remove the start of the sentence token `<s>` from all the sentences.\n",
    "\n",
    "\n",
    "The model's inputs should be followed by a `tf.keras.layers.Embedding()` layer. The `Embedding` layer will act as a lookup table to convert token inputs to their corresponding word2vec values. When initialising the `Embedding` layer, set the layer weights with the `embedding_matrix` you had built in the previous question.\n",
    "\n",
    "Set the trainable to false and mask_zero to true in the `Embedding` layer. The input dimension should be the number of rows in `embedding_matrix` and the output dimension should be your embedding dimension of `300`.\n",
    "    \n",
    "Refer to this image from the lecture slides on ELMo.\n",
    "    \n",
    "    \n",
    "![](https://drive.google.com/uc?export=view&id=1fNPnrBR7Wfh_Jci70QaHde1L3nTcs9-D)\n",
    "    \n",
    "*go_backwards in backward LSTM layers*\n",
    "\n",
    "* To predict the words backwards, we will use the go_backwards parameter present in the TensorFlow LSTM layer implementation. Remember to reverse the output of each backward LSTM layer before using it. Refer to the documentation of [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM). Remember to invert the sequence dimension and not the embedding dimension.\n",
    "\n",
    "Remember to use the **same** *softmax* layer on both the forward and the backward LSTMs.\n",
    "    \n",
    "This will give you an `output_f` and `output_b` which you will evaluate with your two targets.     \n",
    "\n",
    "$$L = -\\sum y \\cdot log(\\widehat{y}_{right}) -\\sum y \\cdot log(\\widehat{y}_{left})$$\n",
    "\n",
    "Use an appropriate loss function, and optimizer and train the network sufficiently.   \n",
    "\n",
    "Finally, plot the training history of the train and validation loss.\n",
    "<br />\n",
    "    \n",
    "**Note**: Use native tensorflow functions like `tf.shape` instead of numpy functions like `np.shape`.\n",
    "<br />  \n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "#### **2.3 [15 points] Transfer Learning**\n",
    "    \n",
    "Once you've sufficiently trained your ELMo embeddings, we can use it for a downstream task such as sentiment analysis.\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3.1** - Baseline model\n",
    "    \n",
    "For the baseline model, you will use:\n",
    "    \n",
    "- `tf.keras.Layers.Embedding()` layer\n",
    "-  2 layers of `GRU` with `hidden_size=300`\n",
    "-  Dense output layer\n",
    "    \n",
    "You will build a `tf.data.Dataset` similar to the one created in Section 2.2.1 but instead of having a target as a series of tokens, the target should only be a class (positive or negative sentiment). Unlike in 2.2.1 we only need a single sequence of tokens for this model\n",
    "\n",
    "Train it for sufficient epochs using an appropriate optimizer and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "\n",
    "**2.3.2** - Directly using pre-trained `word2vec`\n",
    "    \n",
    "For this section, use the pre-trained `word2vec` embeddings directly into your model.\n",
    "\n",
    "You can use the same `tf.data.Dataset` from 2.3.1 for this model\n",
    "    \n",
    "Train, and compare its performance with the baseline model defined above **using the same architecture** as above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
    "    \n",
    "**2.3.3** You have already done sentiment analysis using `tf.keras.layers.Embedding()`. You will now aim to beat that baseline using your ELMo embeddings.\n",
    "\n",
    "Using ELMo embeddings:\n",
    "\n",
    "You will build another `tf.data.Dataset` similar to the one created in Section 2.2.1 but instead of having a target as a series of tokens, the target should only be a class (positive or negative sentiment). This model also requires two inputs one for forward and one for backward LSTM.\n",
    "\n",
    "For this model, you will use:\n",
    "- `Toy_ELMo` model after the input layer\n",
    "-  Sauce layer\n",
    "-  2 layers of `GRU` with `hidden_size=300`\n",
    "-  Dense output layer\n",
    "\n",
    "**NOTE**: Set `Toy_ELMo.trainable` to `False` to avoid retraining the model.\n",
    "        \n",
    "Create the **sauce** layer to combine the three embeddings from your `Toy_ELMo`. You should have **three** trainable parameters in this layer\n",
    "\n",
    "$$ELMo_{t} = \\gamma \\sum_{j=0}^{L} s_{j} h_{t}^{j}$$\n",
    "\n",
    "Since we are not using any other embeddings, we will set the value of $\\gamma$ to 1. <br>\n",
    "Train the modified model sufficiently, and compare it to the previously trained models.\n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper code\n",
    "# Scale layer for sauce\n",
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, shape):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "        self.supports_masking = True\n",
    "        self.shape = shape\n",
    "\n",
    "    def build(self, inputs):\n",
    "        sauce_initializer = tf.keras.initializers.Constant(value = [0.4, 0.3, 0.3])\n",
    "        self.scale = self.add_weight(shape = (self.shape,), initializer = sauce_initializer, trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        scale_norm = tf.nn.softmax(self.scale)\n",
    "\n",
    "        return tf.tensordot(scale_norm, inputs, axes=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
